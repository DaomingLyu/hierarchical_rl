tonight
-------
1. decide between cnn vs flat
2. compressor network (compression loss as incentive, as temp for softmax)
3. recurrent network
4. stacked recurrent network
5. batchnorm
6. prioritized experience replay
    - sliding window -> if sufficiently large, and sampling higher-td experiences, may not need to save in memory for longer 
    - this would require feedback from training to the memory
7. stacked recurrent with different length of BPTT
8. why does the value image have all the same values sometimes?
9. graph the weight updates vs the size of the weights -> should be ratio of 1e-3
10. plot neural network weights / activations / figure out what visualizations work
11. maze just showing current room
    - in the maze problem, just show it the current room!!! and it has to figure out from that
    - need some distinguishing markers for each room? that's available in MR, but not with one-hot room matric
12. maze with key 
13. read papers
14. should the loss behave differently? why does it go to zero so quickly?
    - why is it bimodal or something?
        - bimodal proabably b/c sometime batch contains reward pos and sometimes not?
15. improve visualizations
    - some method of showing how the different runs did
16. do they perform training after every step? or is it every once in a while or what?
17. should I think of a better mdp?
18. some notion of convergence in the test mdps
    - rather than just until "10 in a row or something"
19. clip TD error?
    - should keep track of it 
        - isn't this just the loss?
20. 

---
problems with qnet
---
1. values go to nan sometimes. why? (particularly with rmsprop, does so immediately -> why?)
2. need regularization? 
3. need to make tests checking outputs given set weights and inputs
5. maybe need to be taking random actions till replay memory full
6. check that the weights are sensible after training

---
papers to read (again)
---
- prioritized experience replay
- compressor network 
- hybrid arch
- algorithmic information theory CM
- Memory-based control with recurrent neural networks
- Deep Recurrent Q-Learning for Partially Observable MDPs

