tonight
-------
1. decide between cnn vs flat
2. compressor network (compression loss as incentive, as temp for softmax)
3. recurrent network
4. stacked recurrent network
5. batchnorm
6. prioritized experience replay
7. stacked recurrent with different length of BPTT
8. why does the value image have all the same values sometimes?
9. graph the weight updates vs the size of the weights -> should be ratio of 1e-3
10. plot neural network weights / activations / figure out what visualizations work
11. maze just showing current room
    - in the maze problem, just show it the current room!!! and it has to figure out from that
    - need some distinguishing markers for each room? that's available in MR, but not with one-hot room matric
12. maze with key 
13. read papers
14. should the loss behave differently? why does it go to zero so quickly?
    - why is it bimodal or something?
        - bimodal proabably b/c sometime batch contains reward pos and sometimes not?
15. improve visualizations
    - some method of showing how the different runs did

---
problems with qnet
---
1. values go to nan sometimes. why? (particularly with rmsprop, does so immediately -> why?)
2. need regularization? 
3. need to make tests checking outputs given set weights and inputs
5. maybe need to be taking random actions till replay memory full
6. check that the weights are sensible after training

---
papers to read (again)
---
- prioritized experience replay
- compressor network 
- hybrid arch
- algorithmic information theory CM
- Memory-based control with recurrent neural networks
- Deep Recurrent Q-Learning for Partially Observable MDPs

